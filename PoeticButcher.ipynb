{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PoeticButcher.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1noCc7iOYHwae6uYN5_B47lCA2w1fblfn",
      "authorship_tag": "ABX9TyNeauogDkWYfhLk0EsQYOKv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/SloppyButchery/blob/main/PoeticButcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0hXd6SrnOso"
      },
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">Poetic Butcher <font color=\"#999\" size=\"3\">v 0.0.1<font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font size=\"4\">Sloppy Butchery @</font> <a href=\"https://github.com/olaviinha/SloppyButchery\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font size=\"3\" color=\"#999\"><a href=\"https://inha.se\" target=\"_blank\"><font color=\"#999\">O. Inha</font></a></font></font>\n",
        "\n",
        "Poetic Butcher is a speech slicer. It slices speech to individual words using [Mozilla DeepSpeech](https://github.com/mozilla/DeepSpeech) RNN, and exports each word as a separate WAV file.\n",
        "\n",
        "<font size=\"5\">Howto</font>\n",
        "1. Input a path to an audiofile located in your Google Drive.\n",
        "2. Hit <i>Runtime > Run all</i>.\n",
        "\n",
        "DeepSpeech was not designed for the task at hand and thus is not performing with perfect accuracy. You may want to finetune the start and end time of each word (in milliseconds) with the `start_ms` and `end_ms` sliders.\n",
        "\n",
        "WAV files will be saved under the same directory where your input file is located, in subdirectory `<input_audio>_words`\n",
        "\n",
        "<hr size=\"1\" color=\"#666\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPx0O2yo1XTu",
        "cellView": "form"
      },
      "source": [
        "#@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisities.\n",
        "\n",
        "pip_packages = 'deepspeech-gpu pysoundfile'\n",
        "apt_packages = 'sox'\n",
        "\n",
        "import os\n",
        "from google.colab import output\n",
        "\n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb'):\n",
        "  %cd /content/\n",
        "  !apt-get update -qq && apt-get install -qq {apt_packages}\n",
        "  !pip -q install import-ipynb {pip_packages}\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "  import import_ipynb\n",
        "  from inhagcutils import *\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.isdir('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "# Drive symlink\n",
        "if not os.path.isdir('/content/mydrive'):\n",
        "  os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "  drive_root_set = True\n",
        "drive_root = '/content/mydrive/'\n",
        "\n",
        "# Download models\n",
        "if not 'models_downloaded' in globals():\n",
        "  !curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.8.1/deepspeech-0.8.1-models.pbmm\n",
        "  !curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.8.1/deepspeech-0.8.1-models.scorer\n",
        "  models_downloaded = True\n",
        "\n",
        "dir_tmp = '/content/tmp/'\n",
        "create_dirs([dir_tmp])\n",
        "\n",
        "last_input_audio = ''\n",
        "\n",
        "output.clear()\n",
        "op(c.ok, 'Setup finished.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UURTlViHCFot",
        "cellView": "form"
      },
      "source": [
        "#@title Slice\n",
        "input = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown <small>Finetune the start and end time of each exported WAV (in milliseconds).</small>\n",
        "start_ms = -100 #@param {type:\"slider\", min:-100, max:100, step:5}\n",
        "end_ms = -85 #@param {type:\"slider\", min:-100, max:100, step:5}\n",
        "\n",
        "#@markdown <small>Create preview players inside this notebook –OR– save WAV files directly to your Drive.</small>\n",
        "mode = \"preview\" #@param [\"preview\", \"save_to_drive\"]\n",
        "\n",
        "fade_ms = 3\n",
        "input_audio = drive_root+input_audio\n",
        "id = rnd_str(8)\n",
        "global_sr = 44100\n",
        "global_fade = fade_ms/1000\n",
        "head = start_ms/1000\n",
        "tail = end_ms/1000\n",
        "\n",
        "dir_tmp_input = dir_tmp+'input/'\n",
        "create_dirs([dir_tmp_input])\n",
        "\n",
        "input_type = check_input_type(input)\n",
        "\n",
        "def clip_audio(audio_data, start, duration, sr=global_sr):\n",
        "  global global_fade\n",
        "  xstart = librosa.time_to_samples(start, sr=sr)\n",
        "  xduration = librosa.time_to_samples(start+duration, sr=sr)\n",
        "  audio_data = fade_audio(audio_data[:, xstart:xduration])\n",
        "  return audio_data\n",
        "\n",
        "def fade_audio(audio_data, fade_in=global_fade, fade_out=global_fade, sr=global_sr):\n",
        "  a_duration = librosa.get_duration(audio_data, sr=sr)\n",
        "  if fade_in > 0:\n",
        "    fade_in_to = librosa.time_to_samples(fade_in, sr=sr)\n",
        "    in_y = audio_data[:, 0:fade_in_to]\n",
        "    fade_ins = []\n",
        "    for channel in in_y:\n",
        "      fade = [ i/len(channel)*smp for i, smp in enumerate(channel) ]\n",
        "      fade_ins.append(fade)\n",
        "    fade_ins = np.array(fade_ins)\n",
        "    tail_start = fade_in_to+1  \n",
        "    tail = audio_data[:, tail_start:]\n",
        "    audio_data = np.concatenate([fade_ins, tail], axis=1)\n",
        "  if fade_out > 0:\n",
        "    fade_out_start = librosa.time_to_samples(a_duration-fade_out, sr=sr)\n",
        "    out_y = audio_data[:, fade_out_start:]\n",
        "    fade_outs = []\n",
        "    for channel in out_y:\n",
        "      fade = [ smp-(i/len(channel)*smp) for i, smp in enumerate(channel) ]\n",
        "      fade_outs.append(fade)\n",
        "    fade_outs = np.array(fade_outs)\n",
        "    head_start = fade_out_start-1\n",
        "    head = audio_data[:, :head_start]\n",
        "    audio_data = np.concatenate([head, fade_outs], axis=1)\n",
        "  return audio_data\n",
        "\n",
        "def save(audio_data, save_as, sr=global_sr):\n",
        "  soundfile.write(save_as, audio_data.T, sr)\n",
        "\n",
        "if input_type == 'file':\n",
        "  !cp {input} {dir_tmp_input}\n",
        "  target = dir_tmp_input\n",
        "if input_type == 'dir':\n",
        "  target = input\n",
        "if input_type == 'youtube':\n",
        "  !pip -q install youtube-dl ffmpeg\n",
        "  !youtube-dl --restrict-filenames -x --no-continue --audio-format wav -o \"{dir_tmp_input}%(title)s.%(ext)s\" {input}\n",
        "  target = dir_tmp_input\n",
        "\n",
        "import json, soundfile, librosa\n",
        "\n",
        "file_list = list_audio(target)\n",
        "for audiofile in file_list:\n",
        "  file_id = slug(basename(audiofile))\n",
        "  dir_output = path_dir(input)+file_id+'_words/'\n",
        "  !deepspeech --model=deepspeech-0.8.1-models.pbmm --scorer=deepspeech-0.8.1-models.scorer --audio='{audiofile}' --json --candidate_transcripts=1 > {dir_tmp}{file_id}.json\n",
        "  with open(dir_tmp+file_id'.json', 'r') as j:\n",
        "    json_data = json.load(j)\n",
        "  y, sr = librosa.load(audiofile, mono=False, sr=global_sr)\n",
        "  for i, word in enumerate(json_data['transcripts'][0]['words']):\n",
        "    start = word['start_time'] + head\n",
        "    if start < 0:\n",
        "      start = 0\n",
        "    duration = word['duration'] + abs(head) + tail\n",
        "    word_audio = clip_audio(y, start, duration)\n",
        "    if mode == 'save_to_drive':\n",
        "      save_as = str(i).zfill(4)+'_'+word['word']+'.wav'\n",
        "      save(word_audio, dir_output+save_as)\n",
        "      print('Saved', save_as)\n",
        "    else:\n",
        "      print(word['word'])\n",
        "      audio_player(word_audio)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}