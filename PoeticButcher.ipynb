{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PoeticButcher.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1noCc7iOYHwae6uYN5_B47lCA2w1fblfn",
      "authorship_tag": "ABX9TyPxaxeIqcli+scXiKzY8QgV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/SloppyButchery/blob/main/PoeticButcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0hXd6SrnOso"
      },
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">Poetic Butcher <font color=\"#999\" size=\"3\">v 0.0.3<font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font size=\"4\">Sloppy Butchery @</font> <a href=\"https://github.com/olaviinha/SloppyButchery\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font size=\"3\" color=\"#999\"><a href=\"https://inha.se\" target=\"_blank\"><font color=\"#999\">O. Inha</font></a></font></font>\n",
        "\n",
        "Poetic Butcher is a speech isolator & slicer. It isolates voice from audio source using [Deezer Spleeter](https://github.com/deezer/spleeter) and/or slices it to individual words using [Mozilla DeepSpeech](https://github.com/mozilla/DeepSpeech) RNN.\n",
        "\n",
        "<font size=\"5\">Howto</font>\n",
        "- Enter a path to an audio file located in your Google Drive (relative to Google Drive root) or a link to a Youtube video in the `input` field.\n",
        "- Enter a path to a directory located in your Google Drive (relative to Google Drive root) in the `output_dir` field.\n",
        "- DeepSpeech was not designed for the task at hand and thus is not performing with perfect accuracy. You may want to finetune the start and end time of each word (in milliseconds) with the `head_ms` and `tail_ms` sliders.\n",
        "- If you do voice isolation and your input audio is over 2 minutes long, Colab is unable to provide a preview player within the Notebook. You should use `save_to_drive` mode directly instead.\n",
        "- WAV files will be saved under the same directory where your input file is located, in subdirectory `<input_audio>_words`\n",
        "\n",
        "<hr size=\"1\" color=\"#666\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPx0O2yo1XTu",
        "cellView": "form"
      },
      "source": [
        "#@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisities.\n",
        "\n",
        "pip_packages = 'deepspeech-gpu spleeter pysoundfile'\n",
        "apt_packages = 'ffmpeg sox'\n",
        "\n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb'):\n",
        "  %cd /content/\n",
        "  !apt-get update -qq && apt-get install -qq {apt_packages}\n",
        "  !pip -q install import-ipynb {pip_packages}\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "  !gsutil -q -m cp -R gs://neural-research/olaviinha/spleeter-configs/custom-4stems-22kHz-z.json /content/cfg.json\n",
        "  import import_ipynb\n",
        "  from inhagcutils import *\n",
        "\n",
        "\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.isdir('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "# Drive symlink\n",
        "if not os.path.isdir('/content/mydrive'):\n",
        "  os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "  drive_root_set = True\n",
        "drive_root = '/content/mydrive/'\n",
        "\n",
        "# Download models\n",
        "if not 'models_downloaded' in globals():\n",
        "  !curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.8.1/deepspeech-0.8.1-models.pbmm\n",
        "  !curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.8.1/deepspeech-0.8.1-models.scorer\n",
        "  models_downloaded = True\n",
        "\n",
        "import os\n",
        "from google.colab import output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from spleeter.separator import Separator\n",
        "!gsutil -q -m cp -R gs://neural-research/olaviinha/spleeter-configs/custom-4stems-22kHz-z.json /content/cfg.json\n",
        "print('Fetched gs://neural-research/olaviinha/spleeter-configs/custom-4stems-22kHz-z.json')\n",
        "separator = Separator('/content/cfg.json')\n",
        "\n",
        "dir_tmp = '/content/tmp/'\n",
        "create_dirs([dir_tmp])\n",
        "last_input_audio = ''\n",
        "\n",
        "output.clear()\n",
        "op(c.ok, 'Setup finished.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UURTlViHCFot",
        "cellView": "form"
      },
      "source": [
        "#@title Isolate & Slice\n",
        "input = \"\" #@param {type:\"string\"}\n",
        "output_dir = \"\" #@param {type:\"string\"}\n",
        "\n",
        "isolate_voice = False #@param{type:\"boolean\"}\n",
        "slice_to_words = False #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown <small>Finetune the start and end time of each exported WAV (in milliseconds) when slicing to words.</small>\n",
        "head_ms = -100 #@param {type:\"slider\", min:-100, max:100, step:5}\n",
        "tail_ms = -85 #@param {type:\"slider\", min:-100, max:100, step:5}\n",
        "\n",
        "#@markdown <small>Create preview players inside this notebook –OR– save WAV files directly to your Drive.</small>\n",
        "mode = \"preview\" #@param [\"preview\", \"save_to_drive\"]\n",
        "\n",
        "start_ms = head_ms\n",
        "end_ms = tail_ms\n",
        "\n",
        "dir_output = fix_path(drive_root+output_dir)\n",
        "fade_ms = 3\n",
        "#input_audio = drive_root+input_audio\n",
        "id = rnd_str(8)\n",
        "global_sr = 44100\n",
        "global_fade = fade_ms/1000\n",
        "head = start_ms/1000\n",
        "tail = end_ms/1000\n",
        "\n",
        "dir_tmp_input = dir_tmp+'input/'\n",
        "dir_tmp_spleet = dir_tmp+'spleet/'\n",
        "dir_tmp_words = dir_tmp+'words/'\n",
        "create_dirs([dir_tmp_input, dir_tmp_spleet, dir_tmp_words])\n",
        "\n",
        "input_type = check_input_type(input)\n",
        "\n",
        "def clip_audio(audio_data, start, duration, sr=global_sr):\n",
        "  global global_fade\n",
        "  xstart = librosa.time_to_samples(start, sr=sr)\n",
        "  xduration = librosa.time_to_samples(start+duration, sr=sr)\n",
        "  audio_data = fade_audio(audio_data[:, xstart:xduration])\n",
        "  return audio_data\n",
        "\n",
        "def fade_audio(audio_data, fade_in=global_fade, fade_out=global_fade, sr=global_sr):\n",
        "  a_duration = librosa.get_duration(audio_data, sr=sr)\n",
        "  if fade_in > 0:\n",
        "    fade_in_to = librosa.time_to_samples(fade_in, sr=sr)\n",
        "    in_y = audio_data[:, 0:fade_in_to]\n",
        "    fade_ins = []\n",
        "    for channel in in_y:\n",
        "      fade = [ i/len(channel)*smp for i, smp in enumerate(channel) ]\n",
        "      fade_ins.append(fade)\n",
        "    fade_ins = np.array(fade_ins)\n",
        "    tail_start = fade_in_to+1  \n",
        "    tail = audio_data[:, tail_start:]\n",
        "    audio_data = np.concatenate([fade_ins, tail], axis=1)\n",
        "  if fade_out > 0:\n",
        "    fade_out_start = librosa.time_to_samples(a_duration-fade_out, sr=sr)\n",
        "    out_y = audio_data[:, fade_out_start:]\n",
        "    fade_outs = []\n",
        "    for channel in out_y:\n",
        "      fade = [ smp-(i/len(channel)*smp) for i, smp in enumerate(channel) ]\n",
        "      fade_outs.append(fade)\n",
        "    fade_outs = np.array(fade_outs)\n",
        "    head_start = fade_out_start-1\n",
        "    head = audio_data[:, :head_start]\n",
        "    audio_data = np.concatenate([head, fade_outs], axis=1)\n",
        "  return audio_data\n",
        "\n",
        "def save(audio_data, save_as, sr=global_sr):\n",
        "  soundfile.write(save_as, audio_data.T, sr)\n",
        "\n",
        "if input_type == 'file':\n",
        "  !cp {input} {dir_tmp_input}\n",
        "  target = dir_tmp_input\n",
        "if input_type == 'dir':\n",
        "  target = input\n",
        "if input_type == 'youtube':\n",
        "  !pip -q install youtube-dl\n",
        "  !youtube-dl --restrict-filenames -x --no-continue --audio-format wav -o \"{dir_tmp_input}%(title)s.%(ext)s\" {input}\n",
        "  target = dir_tmp_input\n",
        "\n",
        "import json, soundfile, librosa\n",
        "\n",
        "if isolate_voice == True:\n",
        "  file_list = list_audio(target)\n",
        "  for audiofile in file_list:\n",
        "    separator.separate_to_file(audiofile, dir_tmp_spleet)\n",
        "    old_voice_track = dir_tmp_spleet+'/'+basename(audiofile)+'/vocals.wav'\n",
        "    voice_track = dir_tmp_spleet+'/'+basename(audiofile)+'_voice.wav'\n",
        "    !cp {old_voice_track} {voice_track}\n",
        "    if mode == 'save_to_drive':\n",
        "      save_as = dir_output+basename(audiofile)+'_voice.wav'\n",
        "      !cp {voice_track} {save_as}\n",
        "      op(c.ok, 'File saved', 'as '+save_as.replace(drive_root, ''))\n",
        "    else:\n",
        "      audio_player(voice_track)\n",
        "\n",
        "if slice_to_words == True:\n",
        "  if isolate_voice == True:\n",
        "    file_list = list_audio(dir_tmp_spleet)\n",
        "  else:\n",
        "    file_list = list_audio(target)\n",
        "  for audiofile in file_list:\n",
        "    file_id = slug(basename(audiofile))\n",
        "    dir_output = dir_output+file_id+'_words/'\n",
        "    !deepspeech --model=deepspeech-0.8.1-models.pbmm --scorer=deepspeech-0.8.1-models.scorer --audio='{audiofile}' --json --candidate_transcripts=1 > {dir_tmp}{file_id}.json\n",
        "    with open(dir_tmp+file_id+'.json', 'r') as j:\n",
        "      json_data = json.load(j)\n",
        "    y, sr = librosa.load(audiofile, mono=False, sr=global_sr)\n",
        "    for i, word in enumerate(json_data['transcripts'][0]['words']):\n",
        "      start = word['start_time'] + head\n",
        "      if start < 0:\n",
        "        start = 0\n",
        "      duration = word['duration'] + abs(head) + tail\n",
        "      word_audio = clip_audio(y, start, duration)\n",
        "      if mode == 'save_to_drive':\n",
        "        save_as = str(i).zfill(4)+'_'+word['word']+'.wav'\n",
        "        save(word_audio, dir_output+save_as)\n",
        "        op(c.ok, 'Saved', save_as)\n",
        "      else:\n",
        "        print(word['word'])\n",
        "        audio_player(word_audio)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}