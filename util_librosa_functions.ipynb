{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "util_librosa_functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBVS8X0RI9GBFGVuW5YpA6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/SloppyButchery/blob/main/util_librosa_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y851Z755P-w"
      },
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">Librosa utils <font color=\"#999\" size=\"3\">v 0.0.1<font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font size=\"4\">Sloppy Butchery @</font> <a href=\"https://github.com/olaviinha/SloppyButchery\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font size=\"3\" color=\"#999\"><a href=\"https://inha.se\" target=\"_blank\"><font color=\"#999\">O. Inha</font></a></font></font>\n",
        "\n",
        "Cheatsheet of basic audio-processing functions built around Librosa and Numpy.\n",
        "\n",
        "Please note:\n",
        "- Some of these functions contain residue from original code, as they are copy/pastes from other Sloppy Butchery notebooks, where they actually serve purposes. I will try to clean it up as a functioning, tiny utility library in the future.\n",
        "- Hence, functions are not guaranteed to work \"as is\", but should be easy to modify for development purposes.\n",
        "- Some of these functions also utilize other third party utilities, such as FFMpeg and Deezer Spleeter. `!pip install ffmpeg spleeter`\n",
        "\n",
        "<hr size=\"1\" color=\"#666\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_cM26HE5bTa"
      },
      "source": [
        "import librosa\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqsah2xcizXR"
      },
      "source": [
        "## General"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0_OtCM4izzJ"
      },
      "source": [
        "global_sr = 44100\n",
        "global_fade = 0.003\n",
        "\n",
        "# Convert a file list with FFMPEG\n",
        "# Returns nothing\n",
        "def convert(file_list, output_dir, sr=global_sr):\n",
        "  for i, audiofile in enumerate(file_list):\n",
        "    output = output_dir+slug(path_leaf(basename(audiofile)))+'.wav'\n",
        "    filter = \"pan=stereo|c0=c0|c1=c0\"\n",
        "    if reverse == True:\n",
        "      filter = filter+\", areverse\"\n",
        "    if normalize == True:\n",
        "      filter = filter+\", dynaudnorm=p=1/sqrt(2):m=100:s=12:g=15\"\n",
        "    !ffmpeg {ffmpeg_q} -y -i \"{audiofile}\" -c:a pcm_s16le -ar {sr} -ac 2 -af \"{filter}\" \"{output}\"\n",
        "\n",
        "# Clip a file list\n",
        "# Returns nothing\n",
        "def clip_list(file_list, output_dir, duration, slice_duration, sr=global_sr):\n",
        "  for i, audiofile in enumerate(file_list):\n",
        "    #print('process', audiofile)\n",
        "    #print('clip to', duration)\n",
        "    audio_data, sr = librosa.load(audiofile, sr=sr, mono=False)\n",
        "    a_duration = librosa.get_duration(audio_data, sr=sr)\n",
        "    if a_duration > slice_duration*2:\n",
        "      a_duration = a_duration/4 * random.randrange(2, 3)\n",
        "    start = librosa.time_to_samples(a_duration, sr=sr)\n",
        "    end = librosa.time_to_samples(a_duration+duration+sr, sr=sr)\n",
        "    output = output_dir+path_leaf(audiofile)\n",
        "    save(audio_data[:, start:end], output)\n",
        "    audio_data = None\n",
        "\n",
        "# Get total duration of audio files in a directory\n",
        "# Returns duration in seconds\n",
        "def get_duration(dir, sr=global_sr):\n",
        "  files = list_audio(dir)\n",
        "  duration = 0\n",
        "  for file in files:\n",
        "    duration += librosa.get_duration(filename=file, sr=sr)\n",
        "  return duration\n",
        "\n",
        "# Slice audio signal\n",
        "# Returns slices as audio\n",
        "def slice_to_frames(audio_data, slice_duration, fade_in=global_fade, fade_out=global_fade, fx=[], sr=global_sr):\n",
        "  a_duration = librosa.get_duration(audio_data, sr=sr)\n",
        "  clips = math.ceil(a_duration/slice_duration)\n",
        "  frames = []\n",
        "  for i in range(clips-1):\n",
        "    if i > 0 and i < clips:\n",
        "      start = i*slice_duration\n",
        "      audio_clip = clip_audio(audio_data, start, slice_duration, fx)\n",
        "      frames.append( audio_clip ) #fade_audio(audio_clip, fade_in, fade_out) )\n",
        "  show_mem()\n",
        "  return frames\n",
        "\n",
        "# Clip audio signal\n",
        "# Returns clipped audio siangl\n",
        "def clip_audio(audio_data, start, duration, fx=[], oneshots=False, sr=global_sr):\n",
        "  global global_fade\n",
        "  xstart = librosa.time_to_samples(start, sr=sr)\n",
        "  xduration = librosa.time_to_samples(start+duration, sr=sr)\n",
        "  audio_data = audio_data[:, xstart:xduration]\n",
        "  if len(fx) > 0:\n",
        "    apply_fx(audio_data, duration, fx)\n",
        "  if fx[0] == False and fx[1] == 0:\n",
        "    audio_data = fade_audio(audio_data) \n",
        "  show_mem()\n",
        "  return audio_data\n",
        "\n",
        "# Split stereo audio to left and right\n",
        "# Returns left audio signal, right audio signal\n",
        "def split_channels(audio_data):\n",
        "  return audio_data[0], audio_data[1]\n",
        "\n",
        "# Merge two mono audio signals into stereo audio signal\n",
        "# Returns stereo audio signal\n",
        "def merge_channels(left_data, right_data):\n",
        "  return np.array([left_data, right_data])\n",
        "\n",
        "# Detect pitch of audio signal\n",
        "# Returns pitch in Hz\n",
        "def detect_pitch(audio_data, t, sr=global_sr):\n",
        "  pitches, magnitudes = librosa.core.piptrack(y=audio_data, sr=sr, fmin=50, fmax=900)\n",
        "  # print(pitches)\n",
        "  index = magnitudes[:, t].argmax()\n",
        "  pitch = pitches[index, t]\n",
        "  # print('detect_pitch pitch:', pitch)\n",
        "  return pitch\n",
        "\n",
        "# Generate silence\n",
        "# Returns silent audio signal\n",
        "def generate_silence(duration, sr=global_sr):\n",
        "  content = [0]*librosa.time_to_samples(duration, sr=sr)\n",
        "  silence = np.array([content, content], dtype=np.float32)\n",
        "  return silence\n",
        "\n",
        "# Get spatial periods of audio signal. cycles=return a grain of N spatial periods instead, x_threshold=minimum number of samples between consecutive start times of spatial periods\n",
        "# Returns spatial period start times, spatial period durations\n",
        "def get_spatial_periods(audio, cycles=1, x_threshold=20):\n",
        "  global amount, xtsca, glitch, repeat, rfsca\n",
        "  zero_points = []\n",
        "  last_point = 0\n",
        "  las_val = 0\n",
        "  drop_limit = 0.0005\n",
        "  for i, mg in enumerate(audio):\n",
        "    if i > x_threshold and i < len(audio)-x_threshold:\n",
        "      if round(mg, 1) == 0 and i > last_point+x_threshold and audio[i-1] < drop_limit and audio[i+1] > drop_limit:\n",
        "        zero_points.append(i)\n",
        "        last_point = i\n",
        "  #durations = np.append(np.diff(zero_points), len(audio)-zero_points[-1]) if len(zero_points) else 0\n",
        "  #durations = durations.tolist()\n",
        "  durations = [start - 1 for i, start in enumerate(zero_points) if i > 0]\n",
        "  durations.append(len(audio))\n",
        "  if cycles > 1:\n",
        "    zero_points = zero_points[0::cycles]\n",
        "    durations = durations[0::cycles]\n",
        "  return zero_points, durations\n",
        "\n",
        "# Downsample audio signal to shorter duration\n",
        "# Returns downsampled audio\n",
        "def downsample(audio, new_duration, ends_to_zero=True):\n",
        "  spltr = np.linspace(0, len(audio), num=new_duration+1, dtype=int)\n",
        "  sbr = np.split(audio, spltr[1:])\n",
        "  downsampled = np.array( list( np.mean(item) for item in sbr[:-1] ) )\n",
        "  if ends_to_zero == True:\n",
        "    downsampled[0] = 0\n",
        "    downsampled[-1] = 0\n",
        "  return downsampled\n",
        "\n",
        "# Normalize audio signal\n",
        "# Returns normalized audio\n",
        "def normalize(audio):\n",
        "  return np.interp(audio, (audio.min(), audio.max()), (-1, 1))\n",
        "\n",
        "# Save stereo audio as file\n",
        "def save(audio_data, save_as='frank', sr=global_sr):\n",
        "  if save_as=='frank':\n",
        "    global bpm\n",
        "    timestamp = datetime.datetime.today().strftime('%Y%m%d-%H%M%S')\n",
        "    save_as = save_as+'_'+rnd_str(4)+'_'+timestamp+'__'+bpm+'bpm.wav'\n",
        "  soundfile.write(save_as, audio_data.T, sr)\n",
        "\n",
        "# Save audio as randomly named WAV in TEMP dir, encode to MP3 and create an audio player\n",
        "# Returns audio player in output console\n",
        "def test_audio(audio_data):\n",
        "  global dir_tmp\n",
        "  if not isinstance(audio_data, (np.ndarray, np.generic)):\n",
        "    global global_sr\n",
        "    audio_data, sr = librosa.load(audio_data, mono=False, sr=global_sr)\n",
        "  out = dir_tmp+'test_'+rnd_str(8)\n",
        "  save(audio_data, out+'.wav')\n",
        "  !ffmpeg {ffmpeg_q} -i {out}.wav {mp3_192} {out}.mp3\n",
        "  audio_player(out+'.mp3')\n",
        "\n",
        "# Show RAM usage. Librosa can be quite the memory consumer.\n",
        "# Returns used/available\n",
        "def show_mem():\n",
        "  global extra_verbose_performance\n",
        "  if extra_verbose_performance is True:\n",
        "    print('mem:', psutil.virtual_memory().percent, '/', psutil.virtual_memory().available * 100 / psutil.virtual_memory().total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWVsomyUp38u"
      },
      "source": [
        "## Effects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "391w4jIcp50C"
      },
      "source": [
        "# Remove click from between two concatenated audio clips (to=amplitude of first sample in the following audio signal)\n",
        "# Returns audio signal with end \"morphed\" to amplitude\n",
        "def declick(audio_data, samples=50, to=None):\n",
        "  head = audio_data[:len(data)-samples]\n",
        "  tail = audio_data[len(data)-samples:]\n",
        "  if to == None:\n",
        "    to = head[0]\n",
        "  linear = np.linspace(tail[0], to, samples)\n",
        "  new_tail = []\n",
        "  for i, smp in enumerate(tail):\n",
        "    new_point = smp + (i/samples*linear[i]) - (smp * (i/samples))\n",
        "    new_tail.append(new_point)\n",
        "  return np.concatenate([head, new_tail]).ravel().tolist()\n",
        "\n",
        "# Apply fade in and/or fade out to audio signal\n",
        "# Returns faded audio signal\n",
        "def fade_audio(audio_data, fade_in=global_fade, fade_out=global_fade, sr=global_sr):\n",
        "  a_duration = librosa.get_duration(audio_data, sr=sr)\n",
        "  if fade_in > 0:\n",
        "    fade_in_to = librosa.time_to_samples(fade_in, sr=sr)\n",
        "    in_y = audio_data[:, 0:fade_in_to]\n",
        "    fade_ins = []\n",
        "    for channel in in_y:\n",
        "      fade = [ i/len(channel)*smp for i, smp in enumerate(channel) ]\n",
        "      fade_ins.append(fade)\n",
        "    fade_ins = np.array(fade_ins)\n",
        "    tail_start = fade_in_to+1  \n",
        "    tail = audio_data[:, tail_start:]\n",
        "    audio_data = np.concatenate([fade_ins, tail], axis=1)\n",
        "  if fade_out > 0:\n",
        "    fade_out_start = librosa.time_to_samples(a_duration-fade_out, sr=sr)\n",
        "    out_y = audio_data[:, fade_out_start:]\n",
        "    fade_outs = []\n",
        "    for channel in out_y:\n",
        "      fade = [ smp-(i/len(channel)*smp) for i, smp in enumerate(channel) ]\n",
        "      fade_outs.append(fade)\n",
        "    fade_outs = np.array(fade_outs)\n",
        "    head_start = fade_out_start-1\n",
        "    head = audio_data[:, :head_start]\n",
        "    audio_data = np.concatenate([head, fade_outs], axis=1)\n",
        "  return audio_data\n",
        "\n",
        "# Time-stretch audio signal\n",
        "# Returns time-stretched audio signal\n",
        "def time_stretch_audio(audio, to_length, sr=global_sr):\n",
        "  dur = librosa.get_duration(audio, sr=sr)\n",
        "  #librosa.effects.time_stretch(y, dur/to_length)\n",
        "  return np.array([librosa.effects.time_stretch(channel, dur/to_length) for channel in split_channels(audio)])\n",
        "\n",
        "# Change pitch of audio signal +/- N semitones\n",
        "# Returns pitched audio signal\n",
        "def pitch(audio_data, semitones, sr=global_sr):\n",
        "  pitched = np.array([librosa.effects.pitch_shift(channel, sr=sr, n_steps=semitones, bins_per_octave=12) for channel in split_channels(audio_data)])\n",
        "  audio_data = None\n",
        "  return pitched\n",
        "\n",
        "# Change pitch of audio signal to note\n",
        "# Returns pitched audio signal\n",
        "def autotune_audio(audio_data, note='C', sr=global_sr, t=10):\n",
        "  target_note = librosa.note_to_midi(note)\n",
        "  # print('note', note)\n",
        "  mono_audio = librosa.to_mono(audio_data)\n",
        "  pitch = detect_pitch(mono_audio, t, sr=sr)\n",
        "  # print('pitch hz', pitch)\n",
        "  source_note = round(librosa.hz_to_midi(pitch))\n",
        "  # print('hz_to_midi', midi)\n",
        "  if source_note > 0:\n",
        "    diff = round(target_note-source_note)\n",
        "    oct = 12 if diff > 0 else -12\n",
        "    octs = math.floor(diff/oct)\n",
        "    if octs > 0:\n",
        "      diff = diff-octs*oct\n",
        "    elif octs < 0:\n",
        "      diff = diff+octs*oct\n",
        "\n",
        "    if diff < -6:\n",
        "      if octs < 0:\n",
        "        diff = octs*oct-diff\n",
        "      else:\n",
        "        diff = oct-diff\n",
        "    elif diff > 6:\n",
        "      if octs > 0:\n",
        "        diff = octs*oct+diff\n",
        "      else:\n",
        "        diff = oct-diff \n",
        "    tuned = np.array([librosa.effects.pitch_shift(channel, sr=sr, n_steps=diff, bins_per_octave=12) for channel in split_channels(audio_data)])\n",
        "  else:\n",
        "    tuned = audio_data\n",
        "  mono_audio = None\n",
        "  audio_data = None\n",
        "  return tuned\n",
        "\n",
        "# Apply effects to audio signal, fx=[tremolo, release, autotune, pitch]\n",
        "# Returns audio signal with effects applied\n",
        "def apply_fx(audio_data, duration, fx=[]):\n",
        "  # tremolo\n",
        "  if fx[0] == True:\n",
        "    xtremolo = duration/2\n",
        "    audio_data = fade_audio(audio_data, xtremolo, xtremolo)\n",
        "  #release\n",
        "  elif fx[1] > 0:\n",
        "    xrelease = fx[1]/100*duration\n",
        "    audio_data = fade_audio(audio_data, global_fade, xrelease)\n",
        "  # autotune\n",
        "  if fx[2] != \"None\":\n",
        "    t = math.ceil(10*duration)\n",
        "    if t < 2:\n",
        "      t = 3\n",
        "    audio_data = autotune_audio(audio_data, note=fx[2], t=t)\n",
        "  #pitch\n",
        "  elif fx[3] != 0:\n",
        "    audio_data = pitch(audio_data, fx[3])\n",
        "  return audio_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXWd2vKDix5M"
      },
      "source": [
        "## Beat-slicing related"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOPlBFgFixPA"
      },
      "source": [
        "# Separate drum track with Deezer Spleeter CNN\n",
        "# Returns drum track as mono audio\n",
        "def separate_drums(audio_track):\n",
        "  warnings.filterwarnings('ignore')\n",
        "  separator = Separator('/content/cfg.json')\n",
        "  audio_loader = get_default_audio_adapter()\n",
        "  drum_track = separator.separate(audio_track.T)['drums']\n",
        "  return librosa.to_mono(drum_track.T)\n",
        "\n",
        "# Separate vocal track with Deezer Spleeter CNN\n",
        "# Returns vocal track as mono audio\n",
        "def separate_vocals(audio_track):\n",
        "  warnings.filterwarnings('ignore')\n",
        "  separator = Separator('/content/cfg.json')\n",
        "  audio_loader = get_default_audio_adapter()\n",
        "  vocal_track = separator.separate(audio_track.T)['vocals']\n",
        "  return librosa.to_mono(vocal_track.T)\n",
        "\n",
        "# Detect beat positions in audio signal\n",
        "# Returns starting positions of beats in seconds as a list of numbers\n",
        "def get_beats(audio, sr=global_sr):\n",
        "  bt = BeatTrackerMultiFeature()\n",
        "  beats, _ = bt(audio)\n",
        "  return beats\n",
        "\n",
        "# Calculate differences between a list of numbers (e.g. beat positions)\n",
        "# Returns differences as a list of numbers (e.g. beat durations)\n",
        "def get_differences(blist, round=0):\n",
        "  x = list(np.array(blist.tolist()).flatten())\n",
        "  xdiff = [x[n]-x[n-1] for n in range(1,len(x))]\n",
        "  if round > 0:\n",
        "    rounded = [ '%.2f' % el for el in xdiff ]\n",
        "    return rounded\n",
        "  else:\n",
        "    return xdiff\n",
        "\n",
        "# Get most frequent value from list\n",
        "# Returns most frequent value\n",
        "def most_frequent(list):\n",
        "  freq = max(set(list), key = list.count)\n",
        "  return freq\n",
        "\n",
        "# Get modal value of list\n",
        "# Returns mode\n",
        "def get_mode(lst):\n",
        "  d = {}\n",
        "  for a in lst:\n",
        "    if not a in d:\n",
        "      d[a]=1\n",
        "    else:\n",
        "      d[a]+=1\n",
        "  return [k for k,v in d.items() if v==max(d.values())]\n",
        "\n",
        "# Filter list with a rounded range\n",
        "# Returns filtered list\n",
        "def filter_durations(durations, range, decs):\n",
        "  filtered_durations = []\n",
        "  for duration in durations:\n",
        "    duration = float(duration)\n",
        "    range = float(range)\n",
        "    if round(duration, decs) == range:\n",
        "      filtered_durations.append(duration)\n",
        "  return filtered_durations\n",
        "\n",
        "# Detect peaks at the end of audio signal\n",
        "# Returns fitst tail-peak position or amplitude?\n",
        "def get_tail_peaks(audio, nudgeTime=False, wat=''):\n",
        "  dur = librosa.get_duration(audio, sr=sr)\n",
        "  nudged = False\n",
        "  minPos = 0.96\n",
        "  minPeakDis = 0.001\n",
        "  th = 0.4\n",
        "  pos, amp = PeakDetection(threshold=th, minPeakDistance=minPeakDis, minPosition=minPos)(audio.astype(np.float32))\n",
        "  if len(pos) > 0:\n",
        "    # This slice needs handling\n",
        "    for i, peak in enumerate(pos):\n",
        "      if amp[i] > th and nudged == False:\n",
        "        nudgePeak = peak\n",
        "        nudged = True\n",
        "  if nudgeTime == True and nudged == True:\n",
        "    return nudgePeak\n",
        "  elif nudgeTime == True and nudged == False:\n",
        "    return 0\n",
        "  else:\n",
        "    if wat == 'amp':\n",
        "      return amp\n",
        "    else:\n",
        "      return dur-(pos*dur)\n",
        "\n",
        "# Nudge start times (beat positions) backwards in time until tail peaks are in subsequent slice\n",
        "# Returns new (beat) start positions or audio signals of slices\n",
        "def nudge(beats, timing_track, anal_duration, real_duration, timetravel, return_type='time', starting_beat=0, sr=global_sr):\n",
        "  beat_positions = []\n",
        "  a = 0\n",
        "  for i, beat in enumerate(beats):\n",
        "    if i > starting_beat:\n",
        "      # Get slice\n",
        "      s_time = beat+timetravel\n",
        "      e_time = anal_duration-a\n",
        "      s_sample = librosa.time_to_samples(s_time, sr=sr)\n",
        "      e_sample = librosa.time_to_samples(s_time+e_time, sr=sr)\n",
        "      beat_timing = timing_track[s_sample:e_sample]\n",
        "      # Nudge\n",
        "      nudge_peak = get_tail_peaks(beat_timing, True)\n",
        "      if nudge_peak > 0:\n",
        "        ns_time = s_time-(anal_duration-anal_duration*nudge_peak)-a\n",
        "        ns_sample = librosa.time_to_samples(ns_time, sr=sr)\n",
        "      else:\n",
        "        ns_time = s_time\n",
        "        ns_sample = s_sample\n",
        "      ns_sample = librosa.time_to_samples(ns_time-timetravel, sr=sr)\n",
        "      ne_sample = librosa.time_to_samples(ns_time+(real_duration-a), sr=sr)\n",
        "      # Return start:end of each beat in either 'samples' or in 'seconds'\n",
        "      if return_type == 'samples':\n",
        "        beat_positions.append([ns_sample, ne_sample])\n",
        "      else:\n",
        "        beat_positions.append([ns_time, ns_time+(real_duration-a)])\n",
        "  return beat_positions"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}